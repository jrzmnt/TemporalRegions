# Improving Action Recognition using Temporal Regions

**[This page is still a work in progress!]**

This page contains a supplemental material to the extended version of the paper accepted in the The Symposium on Knowledge Discovery, Mining and Learning (KDMiLe) by Jo√£o P. Aires, Juarez Monteiro, Roger Granada, Felipe Meneguzzi and Rodrigo C. Barros. The full text can be found in [KDMiLe proceedings](http://www.facom.ufu.br/~kdmile/proceedings/anais-kdmile-2017.pdf).

---
## Abstract
Recognizing actions in videos is an important task in computer vision area, having important applications such as the surveillance and assistance of the sick and disabled.
Automatizing this task can improve the way we monitor actions since it is not necessary to have a human watching a video all the time.
However, the classification of actions in a video is challenging since we have to identify temporal features that best represent each action.
In this work, we propose an approach to obtain temporal features from videos by dividing the sequence of frames of a video into regions.
Frames from these regions are merged in order to identify the temporal aspect that classifies actions in a video.
Our approach yields better results when compared to a frame-by-frame classification.

---
## Method

Recognizing actions from videos often involves an analysis of how objects in frames modify along the time.
An approach for action recognition in videos involves obtaining the temporal information, which may contain descriptions about how objects are moving and the main modifications between frames.
Since an action consists of a sequence of movements, obtaining the temporal information may help systems to better understand which action is being performed.
In this work, we propose an approach to obtain temporal information from a video by dividing its frames into regions.
Thus, instead of classifying an action using only the information of each frame, we extract and merge the information from several regions of the video in order to obtain its temporal aspect.
The [Figure](#image) bellow illustrates the pipeline of our architecture, which is divided into four phases: **1. Pre-processing**, **2. CNNs**, **3. Regions Division**, and **4. Classification**.

[image]: images/pipeline.png "Pipeline of our architecture for action recognition using multiple regions"
![Alt text][image]

For more details about the phases you can check our paper at [KDMiLe proceedings](http://www.facom.ufu.br/~kdmile/proceedings/anais-kdmile-2017.pdf).

<!--
### Pre-processing
The pre-processing step intends to adapt raw images from input to fit into our deep learning architectures (AlexNet [[1](#references)] and GoogLeNet [[2](#references)]).
In order to meet the network requirements, we first transform the input image into a square by cropping the largest dimension and maintaining the lowest dimension, *e.g.*, an input image of 460x640 is cropped to 460x460.
The cropped image is then resized to 256x256, which corresponds to the input image of both CNNs.
Resizing is important since it reduces the number of features for each image inside the CNN as well as the processing time.

### CNNs
In this phase, we train two CNNs in order to extract features of the action in each frame.
We train a GoogLeNet [[3](#references)] due to its reduced number of parameters generated by *inception* modules.
The second CNN is a slightly modified version of AlexNet [[4](#references)] as proposed by Ji et al. [[5](#references)].

Both networks receive a sequence of images as input, passing them through several convolutional layers, pooling layers and fully-connected layers (FC), ending in a **softmax** layer that generates the probability of each image to each class.
Thus, we use CNNs as feature extractors for images of the dataset in order to generate a feature representation of each frame.
Such features are the **softmax** result of a forward pass in the trained network, resulting in a vector with the size equals to the number of classes of the dataset.
This vector contains the probability of the input image to belong to each class in classification.

### Regions Division

### Classification
!-->


---
## Experiments
In this work we analyze the performance of algorithms for action recognition using deep learning in small datasets (ranging from a few thousand videos to hundreds of thousands). Each selected dataset differs from others by means of characteristics such as images recorded with a static/dynamic camera, egocentric/third-person view of the camera, *etc*.

### DogCentric dataset

The DogCentric Activity dataset ([DogCentric](http://robotics.ait.kyushu-u.ac.jp/~yumi/db/first_dog.html)) [[4](#references)] consists of first-person videos with outdoor scenes of wearable cameras mounted on dogs' back.

### UCF-11

UCF YouTube Action Dataset ([UCF-11](http://crcv.ucf.edu/data/UCF_YouTube_Action.php)) [[6](#references)] consists of videos that were manually collected from YouTube with a fixed resolution of 240x320 pixels. This dataset is very challenging due to large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions, etc.


---
## Results
In order to evaluate our approach, we calculate accuracy, precision, recall and F-measure that each model achieves.
Since classification accuracy takes into account only the proportion of correct results that a classifier achieves, it is not suitable for unbalanced datasets since it may be biased towards classes with a larger number of examples.
Although other factors may change results, classes with a larger number of examples tend to achieve better results since the network has more examples to learn the variability of the features.
By analyzing both datasets, we notice that they are indeed unbalanced, *i.e.*, classes are not equally distributed over frames.
Hence, we decided to calculate precision, recall, and F-measure to make a better evaluation of the unbalanced dataset.
We calculate scores considering all classes presented in the test set as explained in Section Datasets presented in our paper.

To see tables and plots containing the **results achieved** using the DogCentric and the UCF-11 datasets, click [here](results.md).

---
## How to cite

When citing our work in academic papers, please use this BibTeX entry:

```
@article{aires2017improving,
  title={Improving Activity Recognition using Temporal Regions},
  author={Aires, Jo{\~a}o Paulo and Monteiro, Juarez and Granada, Roger and Meneguzzi, Felipe and Barros, Rodrigo C},
  year={2017}
}
```

---
## References

[1] Simonyan, Karen and Zisserman, Andrew. [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556). Proceedings of the International Conference on Learning Representations (ICLR'15). San Diego, USA, 2015.  

[4] Iwashita, Yumi and Takamine, Asamichi and Kurazume, Ryo and Ryoo, Michael S. [First-Person Animal Activity Recognition from Egocentric Videos](http://dx.doi.org/10.1109/ICPR.2014.739). Proceedings of the 22nd International Conference on Pattern Recognition (ICPR'14), Stockholm, Sweden, pp. 4310-4315, IEEE, 2014.

[6] Liu, Jingen and Luo, Jiebo and Shah, Mubarak. [Recognizing Realistic Actions from Videos "in the Wild"](https://doi.org/10.1109/CVPR.2009.5206744). Proceedings of the 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'09), Miami, FL, USA, pp. 1996-2003, IEEE, 2009.  
